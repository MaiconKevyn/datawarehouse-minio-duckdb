#!/usr/bin/env python3
import os
import duckdb
import pandas as pd
import boto3
from botocore.client import Config
from dotenv import load_dotenv

# ============================================================================= #
# Descri√ß√£o: Script para an√°lise explorat√≥ria de dados de sa√∫de (SIH)
# armazenados como arquivos Parquet no MinIO usando DuckDB como engine
# anal√≠tica em mem√≥ria.
#
# Arquitetura:
# 1. Conex√£o com MinIO (S3-compatible) para invent√°rio dos arquivos
# 2. Configura√ß√£o de DuckDB para leitura federada diretamente do MinIO
# 3. Execu√ß√£o de an√°lises SQL federadas contra os dados brutos
#
# Benef√≠cios desta abordagem:
# - Evita transfer√™ncia desnecess√°ria de dados (data locality)
# - Aproveita a compress√£o e codifica√ß√£o colunar do Parquet
# - Utiliza o poder de processamento do DuckDB para an√°lise in-memory
# - Mant√©m os dados no data lake sem duplica√ß√£o
# ============================================================================= #

# Carrega vari√°veis do ambiente (.env) - padr√£o 12-factor app
load_dotenv()

# Configura√ß√µes do MinIO - Abstra√ß√£o da origem dos dados
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
MINIO_BUCKET = os.getenv("MINIO_BUCKET")
S3_PREFIX = os.getenv("MINIO_S3_PREFIX")


def main():
    # Inicializa cliente S3 para comunica√ß√£o com MinIO
    # Nota: A API S3 √© usada como interface padr√£o para object storage
    s3 = boto3.client(
        's3',
        endpoint_url=MINIO_ENDPOINT,
        aws_access_key_id=MINIO_ACCESS_KEY,
        aws_secret_access_key=MINIO_SECRET_KEY,
        config=Config(signature_version='s3v4'),  # Necess√°rio para compatibilidade
        region_name='us-east-1'
    )

    print(f"üîç Listando arquivos Parquet em s3://{MINIO_BUCKET}/{S3_PREFIX}")

    # Invent√°rio de arquivos Parquet no bucket usando pagina√ß√£o
    # Importante: A pagina√ß√£o garante escalabilidade para buckets com muitos objetos
    parquet_files = []
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=MINIO_BUCKET, Prefix=S3_PREFIX):
        contents = page.get('Contents', [])
        for obj in contents:
            if obj['Key'].endswith('.parquet'):
                parquet_files.append(obj['Key'])
                print(f"- {obj['Key']}")

    if not parquet_files:
        print("Nenhum arquivo Parquet encontrado!")
        return

    # Inicializa√ß√£o do DuckDB em mem√≥ria (in-memory analytical database)
    # Funciona como um data warehouse ef√™mero para an√°lises r√°pidas
    con = duckdb.connect(database=':memory:')

    # Configura√ß√£o do conector S3 do DuckDB para acesso externo
    # Esta configura√ß√£o permite query federation (consultas diretas nos arquivos remotos)
    con.execute(f"""
        SET s3_region='us-east-1';
        SET s3_endpoint='{MINIO_ENDPOINT.replace('http://', '')}';
        SET s3_access_key_id='{MINIO_ACCESS_KEY}';
        SET s3_secret_access_key='{MINIO_SECRET_KEY}';
        SET s3_url_style='path';
        SET s3_use_ssl=false;
    """)

    # Prepara√ß√£o dos caminhos S3 para consulta federada
    # Utilizando path completo para acesso direto aos arquivos Parquet
    s3_paths = [f"s3://{MINIO_BUCKET}/{path}" for path in parquet_files]
    parquet_paths_str = ", ".join(f"'{path}'" for path in s3_paths)

    print("\nüìä Executando consulta nos dados...")

    # An√°lise 1: Estat√≠sticas globais dos dados
    # M√©tricas fundamentais para entender o volume e cardinalidade dos dados
    query = f"""
        SELECT 
            COUNT(*) as total_registros,
            COUNT(DISTINCT N_AIH) as total_AIHs
        FROM read_parquet([{parquet_paths_str}])
    """

    try:
        # Execu√ß√£o da consulta e recupera√ß√£o como DataFrame para manipula√ß√£o
        result = con.execute(query).fetchdf()

        # Apresenta√ß√£o dos resultados iniciais
        print("\n‚úÖ Resultados da consulta:")
        print(result)

        # An√°lise 2: Distribui√ß√£o de frequ√™ncia de procedimentos
        # An√°lise explorat√≥ria para identificar procedimentos mais comuns
        print("\nüìà Top 5 procedimentos mais frequentes:")
        detailed_query = f"""
            SELECT 
                PROC_REA, 
                COUNT(*) as quantidade
            FROM read_parquet([{parquet_paths_str}])
            GROUP BY PROC_REA
            ORDER BY quantidade DESC
            LIMIT 5
        """
        detailed_result = con.execute(detailed_query).fetchdf()
        print(detailed_result)

        # NOTA: Em um ambiente de produ√ß√£o, poder√≠amos expandir com:
        # - Persist√™ncia dos resultados agregados em tabelas materializadas
        # - Gera√ß√£o de visualiza√ß√µes e dashboards automatizados
        # - An√°lises estat√≠sticas mais avan√ßadas (outliers, tend√™ncias, etc.)
        # - Modelos preditivos sobre os dados hist√≥ricos

    except Exception as e:
        print(f"\n‚ùå Erro ao executar a consulta: {e}")
        # Em produ√ß√£o: implementar logging estruturado e sistema de alertas


if __name__ == "__main__":
    main()